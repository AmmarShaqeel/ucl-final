\chapter{Introduction}
Bandwidth demands in data centers have been doubling every 12-15 months. For
data center providers to keep pace with the increased demand (at the same price
point) network switches have had to double their capacity while staying at
roughly the same cost \cite{singh2016jupiter}. However this trend seems to be
coming to an end for two reasons. The first is a predicted increase in the rate
of growth of demand, due to trends like hardware accelerated programming and
dis-aggregated workloads. The second is because electrical switches are
predicted to reach a limit due to the physical limits on pin density
\cite{ballani2018bridging}.

For these reasons optical switching is being explored, as it has the potential
to overcome many of these problems. Optical switches do not require
opto-electrical (OEO) conversion, and hence the number of expensive and power
hungry transceivers required is reduced. Furthermore, as buffering is not
needed, the latency of the optical switches is much lower. Lastly, they do not
use electronics for switching, thus bypassing the aforementioned physical limit
\cite{ballani2018bridging}. 

In data centers much of the traffic that is transmitted between servers is in
the form of small data packets, with 97.8\% of packets being 576 bytes or
less~\cite{data_size}. With 100 Gb/s ports this means that switching should
take place on the order of hundreds of nanoseconds. 

When data is transmitted without a clock signal, the clock has to be
regenerated at the receiver before the data can be decoded - this is known as
clock and data recovery (CDR). The time taken for the local clock to "lock" to
the data stream, adds latency. 
In optical switches physical links are created between each
transceiver-receiver pair. Hence each time the switch is reconfigured, the CDR
must re-lock to the new link.  This means that the network throughput is
limited by the sum of the optical switching time and the CDR locking time -
which can be hundreds of nanoseconds in the worst case and tens of
nanoseconds in the best case~\cite{Chen:18}. Assuming an optical switching time
of 1 nanosecond, it is evident that the CDR locking time acts as bottleneck
that can drastically reduce the throughput~\cite{kari_phase}.

In a source synchronous system the clock is transmitted alongside the data,
removing the CDR locking time. This would remove the bottleneck, theoretically
increasing the throughput.

